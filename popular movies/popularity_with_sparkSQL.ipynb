{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "|    174|  420|\n",
      "|    127|  413|\n",
      "|     56|  394|\n",
      "|      7|  392|\n",
      "|     98|  390|\n",
      "|    237|  384|\n",
      "|    117|  378|\n",
      "|    172|  367|\n",
      "|    222|  365|\n",
      "|    313|  350|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "\n",
      "Star Wars (1977): 583\n",
      "Contact (1997): 509\n",
      "Fargo (1996): 508\n",
      "Return of the Jedi (1983): 507\n",
      "Liar Liar (1997): 485\n",
      "English Patient, The (1996): 481\n",
      "Scream (1996): 478\n",
      "Toy Story (1995): 452\n",
      "Air Force One (1997): 431\n",
      "Independence Day (ID4) (1996): 429\n"
     ]
    }
   ],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"ml-100k/u.ITEM\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"PopularMovies\").getOrCreate()\n",
    "\n",
    "# Load up our movie ID -> name dictionary\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "# Get the raw data\n",
    "lines = spark.sparkContext.textFile(\"file:///SparkCourse/ml-100k/u.data\")\n",
    "print(type(lines))\n",
    "# Convert it to a RDD of Row objects\n",
    "movies = lines.map(lambda x: Row(movieID =int(x.split()[1])))\n",
    "print(type(movies))\n",
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "print(type(movieDataset))\n",
    "\n",
    "\n",
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = movieDataset.groupBy(\"movieID\").count().orderBy(\"count\", ascending=False).cache()\n",
    "\n",
    "# Show the results at this point:\n",
    "\n",
    "#|movieID|count|\n",
    "#+-------+-----+\n",
    "#|     50|  584|\n",
    "#|    258|  509|\n",
    "#|    100|  508|\n",
    "\n",
    "topMovieIDs.show()\n",
    "\n",
    "# Grab the top 10\n",
    "top10 = topMovieIDs.take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n\")\n",
    "for result in top10:\n",
    "    # Each row has movieID, count as above.\n",
    "    print(\"%s: %d\" % (nameDict[result[0]], result[1]))\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
